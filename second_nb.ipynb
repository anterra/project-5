{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow",
   "display_name": "tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import sklearn\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "KERAS_BACKEND=tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.3.0'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vinyasa Krama\n",
    "What follows is my development of the proof-of-concept prototype for an app called Vinyasa Krama, an app which generates complete, well-structured, safe and compelling yoga sequences, via a bi-directional LSTM. In this app, the user is able to pick a desired 'peak' pose -- this is generally the most difficult posture located in the middle of a yoga class, and a custom class will be generated for them which will proper warm them up for and cool them down from that pose, properly preparing the relevant muslces and joints. The class is hence generated from the middle out. They can then take the class, and follow along with an animated yoga teacher (created in the Unity game engine) who will demonstrate the class pose by pose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading In Yoga Class Data\n",
    "\n",
    "Most of these DataFrames were scraped and created during my previous project, \"Yoga Class-ification\", with the exception of vinaysa_df and hatha_df, which I scraped to supplement my existent data to have a larger dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"all_poses\", \"rb\")\n",
    "all_poses = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"flask_app_df\", \"rb\")\n",
    "poses_info = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"all_yoga_classes_df\", \"rb\")\n",
    "yoga_classes = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"poses_df\", \"rb\")\n",
    "class_poses = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"vinyasa_df\", \"rb\")\n",
    "more_vinyasa = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"hatha_df\", \"rb\")\n",
    "more_hatha = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining DataFrames and Excluding Restorative Class Types\n",
    "I am excluding the classes that are of type \"gentle\", \"restorative\" and \"yin\", as these are very slow restorative/laying down type classes that I'm not aiming to create (yet) with this app. I want more dynamic, flow and exercise based classes to be generated, plus I have much more data availble in support of creating those types.\n",
    "\n",
    "Furthermore, I want the LSTM to have rather consistent classes it's being trained on so it can best understand the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([yoga_classes, more_vinyasa, more_hatha])\n",
    "df = df.loc[df[\"Class Type\"].isin([\"Vinyasa\", \"Hatha\", \"Power\", \"Iyengar\", \"Ashtanga\"])]\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   index                               Title  \\\n0      0    ←Slow Sunday Flow y Monday early   \n1      1                           ←Anahata    \n2      2                               ←CORE   \n3      3                                   ←   \n4      4  ←Vinyasa - Bench press & push up #   \n\n                                               Poses Class Type  \n0  [Easy Pose Hands To Heart, Easy Pose Hands Int...    Vinyasa  \n1  [Mantra Section, Thunderbolt Pose, Easy Pose B...    Vinyasa  \n2  [Classic Sun Salutation Variation F, Chair Pos...    Vinyasa  \n3  [Easy Pose, Easy Pose Warm Up Flow, Sun Saluta...    Vinyasa  \n4  [Corpse Pose, Corpse Pose Roll Under Spine, Wr...    Vinyasa  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>Title</th>\n      <th>Poses</th>\n      <th>Class Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>←Slow Sunday Flow y Monday early</td>\n      <td>[Easy Pose Hands To Heart, Easy Pose Hands Int...</td>\n      <td>Vinyasa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>←Anahata</td>\n      <td>[Mantra Section, Thunderbolt Pose, Easy Pose B...</td>\n      <td>Vinyasa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>←CORE</td>\n      <td>[Classic Sun Salutation Variation F, Chair Pos...</td>\n      <td>Vinyasa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>←</td>\n      <td>[Easy Pose, Easy Pose Warm Up Flow, Sun Saluta...</td>\n      <td>Vinyasa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>←Vinyasa - Bench press &amp; push up #</td>\n      <td>[Corpse Pose, Corpse Pose Roll Under Spine, Wr...</td>\n      <td>Vinyasa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Yoga Poses\n",
    "I am concatenating together all of the yoga classes into one long list of lists to be fed to a neural net.\n",
    "\n",
    "I am keeping these as lists, rather than one continuous string, because I want each individual pose to be a token -- if concatenating, NLP methods of tokenization would require I split on white space or some other arbitrary character, which would lose meaning. Instead, I am keeping each pose as an item inside a list, to preserve the meaning of the entire pose name (usually multiple words long) in each token. \n",
    "\n",
    "I am also reducing all of the 3600+ yoga pose variations down to their base poses. Many of the variations are not very distinct from one another, and for the sake of this MVP, the neural net will have richer context for fewer words seen more often, than thousands of different words -- it would not make sense to approach this problem that way, especially since the poses are in fact so similar. \n",
    "\n",
    "I, using industry knowledge, am adjusting from the 102 original base poses in my data, to include a few key variations which actually are distinct and represent different body movements. In this way I am not oversimplifying either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [class_list for class_list in df[\"Poses\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_poses = poses_info[[\"Pose Name\", \"Base Pose\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting this out for readability but I manually looked at every pose in order to create the following...\n",
    "\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "# base_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = [([4, 17, 47, 50, 60], \"Cycling Pose\"),\n",
    "    ([19, 40, 41, 44, 46, 28, 54, 67, 68, 72], \"High Boat To Low Boat Flow\"),\n",
    "    ([64, 66, 69, 70], \"Low Boat Pose\"),\n",
    "    ([86], \"Staff Pose\"), \n",
    "    ([87, 88, 117, 118], \"Bound Angle Forward Bend\"), \n",
    "    ([125, 135, 136, 138, 140, 141], \"One Legged Bow Pose\"), \n",
    "    ([216, 217, 218, 219], \"Pigeon Pose\"), \n",
    "    ([233, 234, 235, 236, 237, 238, 239], \"Thunderbolt Pose\"), \n",
    "    ([248, 269, 270, 271, 274], \"Cow Pose\"), \n",
    "    ([259, 260, 261, 262, 263, 364, 265, 266, 267, 268, 272, 273, 275], \"Cat Pose\"),\n",
    "    ([291], \"Hurdlers Pose\"),\n",
    "    ([293, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 372], \"Revolved Chair Pose\"), \n",
    "    ([300, 303, 306, 307, 308], \"Chair Pose With Airplane Arms\"),\n",
    "    ([365, 369, 370], \"Figure Four Pose\"), \n",
    "    ([368], \"Shiva Squat Pose\"), \n",
    "    ([520, 521], \"Supine Spinal Twist Pose\"), \n",
    "    ([587, 588], \"Flying Pigeon Pose\"), \n",
    "    ([594, 589], \"Side Crow Pose\"), \n",
    "    ([599], \"Baby Crow Pose\"), \n",
    "    ([663, 664, 665, 666, 667, 668], \"One Legged Mountain Pose\"), \n",
    "    ([687, 758], \"One Handed Downward Facing Dog Pose\"),\n",
    "    ([688, 689, 690, 759, 760], \"Standing Splits Pose\"), \n",
    "    ([691, 692, 693, 694, 724, 735, 736, 737, 761, 762, 763, 764], \"Three Legged Downward Facing Dog Pose\"), \n",
    "    ([700, 717], \"Downward Facing Dog Upward Facing Dog Pose Flow\"), \n",
    "    ([714], \"Downward Facing Dog Pose Plank Pose Flow\"), \n",
    "    ([715], \"Downward Facing Dog Pose Table Top Pose Flow\"), \n",
    "    ([747], \"Downward Facing Dog Pose Knee To Nose\"),\n",
    "    ([748], \"Downward Facing Dog Pose Shoulder Taps\"), \n",
    "    ([749], \"Downward Facing Dog Pose to Low Lunge Pose Flow\"), \n",
    "    ([865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901], \"Easy Pose\"), \n",
    "    ([924, 925, 926], \"\"), # this was \"cactus arms\", its a variation that won't make sense in most contexts. intentionally deleting this and will filter out empty strings upon generation. \n",
    "    ([998], \"Half Lotus Pose\"), \n",
    "    ([999], \"Lotus Pose\"), \n",
    "    ([1016, 1017], \"Eye Exercise\"), ([1055, 1056, 1057, 1058, 1061], \"Scorpion Pose\"), \n",
    "    ([1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151], \"Chaturanga Dandasana\"),\n",
    "    ([1387], \"Headstand Pose Eagle Legs\"), \n",
    "    ([1390], \"Headstand Pose Lotus Legs\"), \n",
    "    ([1392], \"Headstand Pose Wide Legs\"), \n",
    "    ([1395, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407], \"Tripod Headstand Pose\"), \n",
    "    ([1464, 1465, 1479, 1480, 1481, 1482], \"Revolved High Lunge Pose\"), \n",
    "    ([1466, 1467, 1468, 1469, 1483], \"Runners Lunge Pose\"), \n",
    "    ([1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533], \"Wide Legged Forward Fold\"), \n",
    "    ([1487, 1509, 1510, 1511, 1512], \"Revolved Wide Legged Forward Fold\"),\n",
    "    ([1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498], \"Wide Legged Forward Fold With Halfway Lift\"),\n",
    "    ([1489, 1490, 1527], \"Five Pointed Star Pose\"), \n",
    "    ([1508], \"Pyramid Pose\"), \n",
    "    ([1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566], \"Pyramid Pose\"), \n",
    "    ([1584], \"Flying Lizard Pose\"), \n",
    "    ([1673, 1684, 1685, 1591, 1700, 1708, 1710, 1711, 1712], \"Revolved Low Lunge Pose\"),\n",
    "    ([1732, 1733, 1745, 1758, 1758, 1776, 1777, 1778, 1784, 1790, 1791, 1791], \"One Legged Moutain Pose\"), \n",
    "    ([1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852], \"Easy Pose\"), \n",
    "    ([1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882], \"Standing Side Bend Pose\"), \n",
    "    ([1891, 1892, 1893, 1901, 1901, 1903, 1904, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922], \"King Pigeon Pose\"), \n",
    "    ([1927, 1928, 1932], \"Grasshopper Pose\"), \n",
    "    ([1929], \"Dragonfly Pose\"), \n",
    "    ([1930], \"Eight Angle Pose\"),\n",
    "    ([1934, 1970, 1971, 1972], \"One Legged Plank Pose\"), ([1939], \"Side Plank Pose\"), \n",
    "    ([1958, 1959, 1960, 1961, 1967, 1968, 1969], \"Forearm Plank Pose\"), \n",
    "    ([2173], \"Lotus Pose\"), \n",
    "    ([2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200], \"Revolved Extended Side Angle Pose\"), \n",
    "    ([2189, 2195, 2196], \"Revolved High Lunge Pose\"), \n",
    "    ([2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368], \"Skandasana\"),\n",
    "    ([2390, 2397], \"Anantasana\"), \n",
    "    ([2415, 2416, 2417, 2418, 2419, 2420, 2443], \"Visvamitrasana Pose\"), \n",
    "    ([2422, 2427, 2428, 2430, 2431, 2433, 2453, 2458, 2460], \"Side Plank Pose With Leg Variation\"), \n",
    "    ([2436, 2437, 2438, 2439, 2440, 2441], \"Forearm Side Plank Pose\"), \n",
    "    ([2465, 2466], \"Wild Thing Pose\"), \n",
    "    ([2482, 2483, 2484, 2486, 2487, 2488, 2489, 2480, 2492, 2493], \"Half Splits Pose\"), \n",
    "    ([2494], \"Standing Splits Pose\"), \n",
    "    ([2592, 2593, 2594], \"Firefly Pose\"), \n",
    "    ([2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621], \"Forward Fold Pose With Halfway Lift\"), \n",
    "    ([2688, 2689, 2692, 2717], \"Bird Of Paradise Pose\"), \n",
    "    ([2700, 2701, 2703], \"Revolved Hand To Big Toe Pose\"), \n",
    "    ([2750, 2758], \"Half Sun Salutation\"), \n",
    "    ([2759], \"Second Half Of Sun Salutation\"), \n",
    "    ([2771, 2772], \"Banana Pose\"), \n",
    "    ([2795], \"Supine Spinal Twist Pose\"), \n",
    "    ([2797, 2798, 2833, 2848, 2849, 2850], \"Tiger Pose\"), \n",
    "    ([2799, 2816, 2827], \"Table Top Knee To Nose Flow\"), \n",
    "    ([2800, 2817], \"Child Pose Table Top Pose Flow\"), \n",
    "    ([2801, 2831, 2834, 2837, 2840, 2841, 2842], \"Table Top Pose With One Leg Extended Back\"), \n",
    "    ([2809, 2810, 2811, 2812, 2813, 2814, 2815], \"Revolved Table Top Pose\"), \n",
    "    ([2852, 2853, 2854, 2855], \"Table Top Balancing Pose, Opposite Arm and Leg Extended\"), \n",
    "    ([2968, 2972, 2983, 2974, 2975, 2976, 2977, 3005, 3006, 3007, 3008], \"Revolved Triangle Pose\"), \n",
    "    ([3023, 3024, 3025, 3026, 3031, 3032, 3038, 3039, 3040], \"Reverse Table Top Pose\"), \n",
    "    ([3037], \"Flip The Dog Pose\"), \n",
    "    ([3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063], \"Anantasana\"), \n",
    "    ([3064, 3065, 3066, 3067, 3070, 3071, 3072, 3073, 3074, 3075, 3079], \"Warrior Pose I\"), \n",
    "    ([3068, 3069, 3076, 3077, 3078], \"Humble Warrior Pose\"), \n",
    "    ([3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121], \"Warrior Pose II\"), \n",
    "    ([3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144], \"Warrior Pose III\"), \n",
    "    ([3134], \"Shiva Squat Pose\"), \n",
    "    ([3131, 3132, 3133], \"Airplane Pose\"), \n",
    "    ([3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190], \"Half Wind Release Pose\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in changes:\n",
    "    base_poses.loc[i[0], \"Base Pose\"] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "170"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(base_poses[\"Base Pose\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know have 170 distinct poses that I will be feeding to my neural net. Now creating a list of class data to feed to the neural net out of the base documents only:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_documents = []\n",
    "for i in range(len(documents)):\n",
    "    temp_df = pd.DataFrame(documents[i], columns=[\"0\"])\n",
    "    temp_df = pd.merge(temp_df, base_poses, how=\"left\", left_on=\"0\", right_on=\"Pose Name\")\n",
    "    temp_df = temp_df.dropna(how=\"any\")\n",
    "    new_doc = [pose for pose in temp_df[\"Base Pose\"]]\n",
    "    base_documents.append(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Start and End Poses\n",
    "For the sake of class generation, since generation is starting with the user's desired most difficult pose in the middle of the class, I want the classes to not be generated to some arbitary pre-set length, but instead continue generating until converging to the natural start and end yoga poses of a class. Hence I am finding the most common start and end poses across all classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_poses = []\n",
    "for doc in base_documents:\n",
    "    if doc:\n",
    "        first_poses.append(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_poses = []\n",
    "for doc in base_documents:\n",
    "    if doc:\n",
    "        last_poses.append(doc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Easy Pose : 12182\nThunderbolt Pose : 743\nSun Salutation : 1764\nCorpse Pose : 5272\nTable Top Pose : 669\nLotus Pose : 433\nPranayama : 2099\nReclined Bound Angle Pose : 1207\nChild Pose : 2534\nCycling Pose : 326\n"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(list(Counter(first_poses).keys())[i], \":\", list(Counter(first_poses).values())[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Corpse Pose : 20486\nSide Lying Corpse Pose : 637\nDownward Facing Dog Pose : 684\nThunderbolt Pose : 193\nHalf Sun Salutation : 49\nThread The Needle Pose : 117\nRevolved Wide Legged Forward Fold : 32\nBridge Pose : 630\nPranayama : 971\nChair Pose : 280\n"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(list(Counter(last_poses).keys())[i], \":\", list(Counter(last_poses).values())[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of classes begin with easy pose (simple cross-legged seat, often for meditation). Pranayama which is also very high just means \"breathing\", which is not itself a \"pose\", but a breath technique which would commonly practiced while in sitting meditation (easy pose). By far the most common last pose is 'corpse pose', or final savasana, lying down meditation. \n",
    "\n",
    "Without even having done the above exploration, I would have been inclined to start the classes with 'easy pose' and end with 'corpse' pose just due to my personal experience with yoga, but now I have the empirical evidence to support that decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings\n",
    "I am using a custom word embedding model to find similarities between poses. I will then implement transfer learning, using this pre-trained word embedding model as my initial weights for the LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gensim.models.Word2Vec(base_documents, size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "169"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "embeddings_size = len(list(embeddings.wv.vocab.items()))\n",
    "embeddings_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Embedding Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.098134644\n0.1699742\n0.39498302\n0.81561786\n"
    }
   ],
   "source": [
    "print(embeddings.similarity(\"Corpse Pose\", \"Dancer Pose\"))\n",
    "print(embeddings.similarity(\"Corpse Pose\", \"Extended Side Angle Pose\"))\n",
    "print(embeddings.similarity(\"Corpse Pose\", \"Child Pose\"))\n",
    "print(embeddings.similarity(\"Corpse Pose\", \"Wind Release Pose\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks very accurate. When compared to 'corpse pose', which is lying down meditation, model indicates very little similarity to difficulty balancing and strength-heavy standing poses, mild similarity to an intense prone stretch pose, moderate similarity to a relaxation prone pose, and very high similarity to wind release pose, which often tends to be practiced immediately before corpse pose, and is also very easy and practiced while laying on the back. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(worda, wordb, wordc):\n",
    "    result = embeddings.most_similar(negative=[worda], positive=[wordb, wordc])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Table Top Balancing Pose, Opposite Arm and Leg Extended'"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "analogy(\"Downward Facing Dog Pose\", \"Three Legged Downward Facing Dog Pose\", \"Table Top Pose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Palm Tree Pose'"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "analogy(\"Warrior Pose II\", \"Extended Side Angle Pose\", \"Mountain Pose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Chair Pose'"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "analogy(\"Revolved Triangle Pose\", \"Triangle Pose\", \"Revolved Chair Pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analogies also indicate that the embeddings model has picked up a good deal of nuanced meaning about the poses. When down dog to a balancing version of downdog is compared to table top, it offers a balancing version of table top. When an arms extended version of warrior II is compared to mountain pose, it offers an arms extended version of mountain (palm tree). When a revolved version of triangle and regular triangle is compared to a revolved version of chair, it offers regular chair. Wonderful! \n",
    "\n",
    "Saving the embeddings as a matrix for use in the neural net: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(embeddings.wv.vocab)+1, 100))\n",
    "for i in range(len(embeddings.wv.vocab)):\n",
    "    embedding_vector = embeddings.wv[embeddings.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = embeddings.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional LSTM Neural Network\n",
    "\n",
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total tokens:  1624681\nUnique tokens:  169\n"
    }
   ],
   "source": [
    "tokens = [pose for yogaclass in base_documents for pose in yogaclass]\n",
    "print(\"Total tokens: \", len(tokens))\n",
    "print(\"Unique tokens: \", len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Sequences\n",
    "I am training the network on an input sequence length of 3 poses, where it will then learn the expected fourth pose. This small input sequence length size is ideal for seeing all the poses in many many contexts, and allows for easy generation from a selected single base pose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total Sequences: 1624670\n"
    }
   ],
   "source": [
    "length = 3 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    sequences.append(seq)\n",
    "print(\"Total Sequences:\", len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Vocab Size: 170\n"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocab Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([np.asarray(x[:-1]) for x in token_sequences])\n",
    "y = [y[-1] for y in token_sequences]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length=len(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Architecture \n",
    "My choice of deep learning model is a bi-directional LSTM. This is the ideal choice for my purposes. \n",
    "\n",
    "Long Short-Term Memory NNs are considered the best performing models for sequence prediction, and are excellent for text generation -- and I am, in this project, doing both. I am generating text that is a sequence of yoga poses that should go in a fairly specific order in order to be safe and comprise a good class. \n",
    "\n",
    "A bi-directional LSTM is an even more ideal choice -- Since I am generating my classes from the middle (peak pose) out, I essentially need to predict and generate half of the from the peak to the end in a forward direction, and the other half of the sequence from the peak to the beginning in a backwards direction. A bi-directional LSTM is uniquely poised to allow me to do this -- by providing the input sequence to each layer twice, once as is and once reversed, and then concatenating the outputs -- the msdel has twice as much context about each pose, and is then able to predict what pose would *precede* a given sequence, as well as follow it. \n",
    "\n",
    "I begin with using transfer learning, initializing my weights with my word embeddings model, of which I set trainable to 'False' so that back propagation does not change the embeddings themselves. I have chosen after some experimentation to then use 3 hidden bidirectional LSTM layers of a size roughly comparable to my vocabulary, with dropout layers between each. I then have 2 activation layers, the first using RELU as the activation function and the second using softmax, as any form of sequence/text prediction is categorized as a multi-class classification problem. \n",
    "\n",
    "I train the model with a modest batch size of 128 (given the 1.6mil input sequences), over 100 epochs, using the adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Bidirectional(LSTM(200, return_sequences=True)))\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(Bidirectional(LSTM(200, return_sequences=True)))\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(Bidirectional(LSTM(200)))\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X, y):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X, y, batch_size=128, epochs=100, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 100)         17000     \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, None, 400)         481600    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, None, 400)         0         \n_________________________________________________________________\nbidirectional_4 (Bidirection (None, None, 400)         961600    \n_________________________________________________________________\ndropout_4 (Dropout)          (None, None, 400)         0         \n_________________________________________________________________\nbidirectional_5 (Bidirection (None, 400)               961600    \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 400)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 100)               40100     \n_________________________________________________________________\ndense_3 (Dense)              (None, 170)               17170     \n=================================================================\nTotal params: 2,479,070\nTrainable params: 2,462,070\nNon-trainable params: 17,000\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "lstm_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\nEpoch 2/100\nEpoch 3/100\nEpoch 4/100\nEpoch 5/100\nEpoch 6/100\nEpoch 7/100\nEpoch 8/100\nEpoch 9/100\nEpoch 10/100\nEpoch 11/100\nEpoch 12/100\nEpoch 13/100\nEpoch 14/100\nEpoch 15/100\nEpoch 16/100\nEpoch 17/100\nEpoch 18/100\nEpoch 19/100\nEpoch 20/100\nEpoch 21/100\nEpoch 22/100\nEpoch 23/100\nEpoch 24/100\nEpoch 25/100\nEpoch 26/100\nEpoch 27/100\nEpoch 28/100\nEpoch 29/100\nEpoch 30/100\nEpoch 31/100\nEpoch 32/100\nEpoch 33/100\nEpoch 34/100\nEpoch 35/100\nEpoch 36/100\nEpoch 37/100\nEpoch 38/100\nEpoch 39/100\nEpoch 40/100\nEpoch 41/100\nEpoch 42/100\nEpoch 43/100\nEpoch 44/100\nEpoch 45/100\nEpoch 46/100\nEpoch 47/100\nEpoch 48/100\nEpoch 49/100\nEpoch 50/100\nEpoch 51/100\nEpoch 52/100\nEpoch 53/100\nEpoch 54/100\nEpoch 55/100\nEpoch 56/100\nEpoch 57/100\nEpoch 58/100\nEpoch 59/100\nEpoch 60/100\nEpoch 61/100\nEpoch 62/100\nEpoch 63/100\nEpoch 64/100\nEpoch 65/100\nEpoch 66/100\nEpoch 67/100\nEpoch 68/100\nEpoch 69/100\nEpoch 70/100\nEpoch 71/100\nEpoch 72/100\nEpoch 73/100\nEpoch 74/100\nEpoch 75/100\nEpoch 76/100\nEpoch 77/100\nEpoch 78/100\nEpoch 79/100\nEpoch 80/100\nEpoch 81/100\nEpoch 82/100\nEpoch 83/100\nEpoch 84/100\nEpoch 85/100\nEpoch 86/100\nEpoch 87/100\nEpoch 88/100\nEpoch 89/100\nEpoch 90/100\nEpoch 91/100\nEpoch 92/100\nEpoch 93/100\nEpoch 94/100\nEpoch 95/100\nEpoch 96/100\nEpoch 97/100\nEpoch 98/100\nEpoch 99/100\nEpoch 100/100\n"
    }
   ],
   "source": [
    "fit_model(lstm_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yoga Class Generation \n",
    "\n",
    "Here I define a function for generating a yoga class from the trained neural network. \n",
    "\n",
    "From the user's chosen peak pose, I create a seed text of input length 3 (NN's input sequence length) by randomly selecting 2 out of top 10 most related poses to the peak pose via the word embeddings model. \n",
    "\n",
    "Rather than end when reaching some arbitrary pre-defined length, I instead allow the class to coninue generating until it naturally converges upon the logical entry and exit poses to a yoga class, 'easy pose' and 'corpse pose', respectively. \n",
    "\n",
    "As the class grows, I chose not to continuously trip and pad the sequence to be only 3 poses long, because I found that the quality and logic of the classes being created was improved when allowing the input to grow. \n",
    "\n",
    "The neural net is typically very good at repeating a pose twice in the sequence if it is an imbalanced pose -- i.e. if it is non-symmetrical across both sides of the body, it should be repeated once for each side of the body. If a pose is repeated, I hence manually add clarification text for user to repeat the pose on the other side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class(model, tokenizer, word_embedding, peak_pose, stop_word, max_length):\n",
    "\n",
    "    # generate seed text\n",
    "    seed_text = [peak_pose, embeddings.most_similar(peak_pose, topn=10)[np.random.choice(range(10))][0], embeddings.most_similar(peak_pose, topn=10)[np.random.choice(range(10))][0]]\n",
    "    in_text = seed_text\n",
    "\n",
    "    # create yoga class, explicitly including user's desired peak pose\n",
    "    yoga_class = list()\n",
    "    yoga_class.append(peak_pose.lower())\n",
    "\n",
    "    # generate sequence \n",
    "    while True:\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])\n",
    "\n",
    "        # select next pose integer based on models probability distribution\n",
    "        prediction_output = model.predict(encoded)\n",
    "        print(prediction_output)\n",
    "        yhat = np.random.choice(len(prediction_output[0]), p=prediction_output[0])\n",
    "        # pred_proba = model.predict(encoded)[::-1][:10]\n",
    "        # # print(\"pred proba:\", pred_proba)\n",
    "        # pred_proba = np.argsort(model.predict(encoded))[0:10]\n",
    "        #pred_proba_norm = pred_proba / np.sum(pred_proba) # normalize\n",
    "\n",
    "        #next_index = sample()\n",
    "        #probs = np.argsort(model.predict(encoded))[::-1][:10]\n",
    "        #yhat = np.random.choice(len(probs[0]), p=probs[0])\n",
    "        # yhat = np.argsort(model.predict(encoded))[::-1][:10]\n",
    "\n",
    "        # find pose in dictionary \n",
    "        out_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        \n",
    "        \n",
    "        # append pose to current class, and update input text\n",
    "        # add 'repeat other side' text on duplicate poses\n",
    "        if out_word != \"\":\n",
    "            in_text.append(out_word)\n",
    "            if out_word == yoga_class[-1]:\n",
    "                if stop_word == \"easy pose\":\n",
    "                    yoga_class[-1] += \", repeat other side\"\n",
    "                    yoga_class.append(out_word)\n",
    "                if stop_word == \"corpse pose\":\n",
    "                    out_word += \", repeat other side\"\n",
    "                    yoga_class.append(out_word)\n",
    "            else: \n",
    "                yoga_class.append(out_word)\n",
    "        if out_word == stop_word:\n",
    "            break\n",
    "\n",
    "\n",
    "        # if sequence gets too long without converging to a natural ending, start over and try again. \n",
    "        if len(yoga_class) == max_length:\n",
    "                in_text = seed_text\n",
    "                yoga_class = [peak_pose.lower()]\n",
    "\n",
    "    yoga_class = [i.title() for i in yoga_class]\n",
    "    return yoga_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Peak Poses \n",
    "Here I define a list of possible peak poses for the user to choose from. I further break them down into their difficulty level, so that the user can select a pose that suits their ability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_peak_poses = ['Center Splits Pose', 'Hurdlers Pose', 'Revolved Chair Pose', 'Figure Four Pose', 'Crane Pose', 'Flying Pigeon Pose', 'Side Crow Pose', 'Baby Crow Pose', 'Crow Pose', 'Dancer Pose', 'Eagle Pose', 'Feathered Peacock Pose', 'Scorpion Pose', 'Flamingo Pose', 'Foot Behind The Head Pose', 'Half Moon Pose', 'Handstand Pose', 'Headstand Pose', 'Headstand Pose Eagle Legs', 'Headstand Pose Lotus Legs', 'Headstand Pose Wide Legs', 'Tripod Headstand Pose', 'Revolved High Lunge Pose', 'Pyramid Pose', 'Flying Lizard Pose', 'Palm Tree Pose', 'King Pigeon Pose', 'Grasshopper Pose', 'Dragonfly Pose', 'Eight Angle Pose', 'Revolved Extended Side Angle Pose', 'Visvamitrasana Pose', 'Splits Pose', 'Firefly Pose', 'Bird Of Paradise Pose', 'Standing Hand To Big Toe Pose', 'Revolved Hand To Big Toe Pose', 'Tree Pose', 'Warrior Pose I', 'Humble Warrior Pose', 'Warrior Pose II', 'Warrior Pose III', 'Airplane Pose', 'Wheel Pose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_levels = poses_info[[\"Pose Name\", \"Level\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df = pd.DataFrame(possible_peak_poses, columns=[\"Peak Pose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_levels = pd.merge(left=peak_df, right=pose_levels, how=\"left\", left_on=\"Peak Pose\", right_on=\"Pose Name\")[[\"Peak Pose\", \"Level\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             Peak Pose     Level\n0   Center Splits Pose  Advanced\n1        Hurdlers Pose       NaN\n2  Revolved Chair Pose  Beginner\n3     Figure Four Pose       NaN\n4           Crane Pose  Advanced",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Peak Pose</th>\n      <th>Level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Center Splits Pose</td>\n      <td>Advanced</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hurdlers Pose</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Revolved Chair Pose</td>\n      <td>Beginner</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Figure Four Pose</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Crane Pose</td>\n      <td>Advanced</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "peak_levels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I added some custom variations to poses, there will be some NaN values as seen above. As such, I'll have to manually add in their difficulty levels using industry knowledge: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_levels = [([1], \"Advanced\"), ([3], \"Beginner\"), ([14], \"Advanced\"), ([21], \"Intermediate\"), ([23], \"Beginner\"), ([26], \"Advanced\"), ([27], \"Advanced\"), ([28], \"Advanced\"), ([30], \"Intermediate\"), ([31], \"Advanced\"), ([34], \"Advanced\"), ([35], \"Intermediate\"), ([36], \"Intermediate\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in more_levels:\n",
    "    peak_levels.loc[i[0], \"Level\"] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             Peak Pose     Level\n0   Center Splits Pose  Advanced\n1        Hurdlers Pose  Advanced\n2  Revolved Chair Pose  Beginner\n3     Figure Four Pose  Beginner\n4           Crane Pose  Advanced",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Peak Pose</th>\n      <th>Level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Center Splits Pose</td>\n      <td>Advanced</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hurdlers Pose</td>\n      <td>Advanced</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Revolved Chair Pose</td>\n      <td>Beginner</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Figure Four Pose</td>\n      <td>Beginner</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Crane Pose</td>\n      <td>Advanced</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 214
    }
   ],
   "source": [
    "peak_levels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_beg = peak_levels.loc[peak_levels[\"Level\"] == \"Beginner\"]\n",
    "peak_int = peak_levels.loc[peak_levels[\"Level\"] == \"Intermediate\"]\n",
    "peak_adv = peak_levels.loc[peak_levels[\"Level\"] == \"Advanced\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_pose_dict = dict()\n",
    "\n",
    "peak_pose_dict[\"Beginner\"] = list(peak_beg[\"Peak Pose\"])\n",
    "peak_pose_dict[\"Intermediate\"] = list(peak_int[\"Peak Pose\"])\n",
    "peak_pose_dict[\"Advanced\"] = list(peak_adv[\"Peak Pose\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Class Generation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_peak():\n",
    "    peak_pose = np.random.choice(possible_peak_poses)\n",
    "    return peak_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Figure Four Pose'"
     },
     "metadata": {},
     "execution_count": 366
    }
   ],
   "source": [
    "peak_pose = random_peak()\n",
    "peak_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9 0.00595199 0.00584414\n  0.00585291 0.0058134  0.00574711 0.00574707 0.00598754 0.00579765\n  0.00583045 0.00586635 0.00578122 0.00589227 0.0058205  0.0059119\n  0.00578942 0.00571103 0.00572234 0.00576266 0.0057761  0.00592246\n  0.00590068 0.0058374  0.00570826 0.00582424 0.00595496 0.00576508\n  0.00598765 0.00610566 0.00591828 0.00565958 0.00597509 0.00577833\n  0.00583945 0.00573555 0.00606003 0.0060163  0.00578057 0.00595729\n  0.00588216 0.00583369 0.00600925 0.00595141 0.00587931 0.00589924\n  0.0057338  0.00606484 0.00567203 0.00579106 0.00586127 0.00584708\n  0.00574416 0.00591035 0.00585218 0.00594864 0.00588913 0.00592875\n  0.00587629 0.00592163 0.00579282 0.00586296 0.00591023 0.00604492\n  0.00586399 0.00580987]]\n[[0.00578358 0.00572062 0.00581341 0.00588262 0.00589919 0.00598347\n  0.00562468 0.00611749 0.00575587 0.00567359 0.00607557 0.00582061\n  0.00594905 0.00587383 0.00591041 0.00584842 0.00573602 0.00601728\n  0.00591082 0.00584562 0.00601645 0.00592865 0.00579089 0.00602653\n  0.0058359  0.00592263 0.00604935 0.00571851 0.00569166 0.00580734\n  0.005757   0.00594954 0.00572971 0.0058225  0.00594172 0.00620674\n  0.00610852 0.00574261 0.00594023 0.00600232 0.00575542 0.00579443\n  0.00589377 0.00596533 0.00596297 0.00592822 0.00590011 0.00584487\n  0.00603091 0.00595389 0.00594987 0.00613092 0.00600833 0.00602412\n  0.0058846  0.00582339 0.00566923 0.00588871 0.00595022 0.00597162\n  0.00582618 0.00568763 0.00574084 0.00596359 0.00594128 0.00587457\n  0.00585637 0.00586117 0.00588583 0.00576742 0.00568852 0.00580567\n  0.00585747 0.00593486 0.0058518  0.00602575 0.0059439  0.0058121\n  0.00588741 0.00613135 0.00595022 0.00590645 0.00597771 0.00586393\n  0.00594131 0.00593183 0.00597176 0.005945   0.00579223 0.00571815\n  0.0059397  0.00596425 0.00578371 0.00580032 0.00584221 0.00591463\n  0.00587996 0.00597322 0.00602638 0.00614494 0.00591063 0.00602131\n  0.0059283  0.00583175 0.00590338 0.00601579 0.00594202 0.00586125\n  0.00587372 0.00582347 0.00575152 0.0057562  0.00598768 0.00579308\n  0.0058319  0.00587035 0.0057708  0.00588082 0.00582793 0.00592047\n  0.00579553 0.00572126 0.00572919 0.0057474  0.0057939  0.00592861\n  0.00589723 0.00583802 0.00572815 0.00584511 0.00596375 0.00576182\n  0.00597758 0.00608235 0.00591396 0.00567324 0.00596301 0.00578297\n  0.0058354  0.00575105 0.0060404  0.00602728 0.00578406 0.00596168\n  0.00589049 0.00581337 0.00599837 0.00596556 0.00586367 0.00590793\n  0.00576645 0.00604358 0.0056728  0.00579164 0.00585422 0.00584019\n  0.00574743 0.00590253 0.00585146 0.00593409 0.00588777 0.00593749\n  0.00586716 0.00594445 0.00580818 0.00586913 0.00589438 0.00603304\n  0.00586582 0.00582991]]\n[[0.00578589 0.0057233  0.00580639 0.00587347 0.00591399 0.00597957\n  0.0056357  0.00609854 0.00577304 0.00569209 0.00606026 0.00582236\n  0.00594797 0.00587751 0.00591632 0.00583583 0.00574104 0.00602364\n  0.00590922 0.00584685 0.00600976 0.00593218 0.00579495 0.00600758\n  0.00583014 0.00592309 0.00605782 0.00573776 0.00568397 0.00580763\n  0.00577065 0.00593233 0.00572845 0.00583232 0.00594209 0.00619079\n  0.00609598 0.00575322 0.00593254 0.00600805 0.00577111 0.00580767\n  0.00590105 0.00596259 0.00597253 0.00591178 0.00590175 0.00583056\n  0.00601907 0.00594273 0.00592987 0.00611686 0.00599533 0.00601178\n  0.00588726 0.00582577 0.00568717 0.00590198 0.00593004 0.00597664\n  0.0058313  0.00569984 0.00573028 0.00596368 0.00593564 0.00587501\n  0.0058508  0.00587004 0.00588566 0.0057695  0.00570103 0.00579487\n  0.00586023 0.00594769 0.00586584 0.00600596 0.00594108 0.0058225\n  0.00588512 0.0061372  0.00594438 0.00589746 0.00597662 0.005862\n  0.00594051 0.00593513 0.00596463 0.0059434  0.00578887 0.00570977\n  0.00591068 0.00597053 0.00578716 0.00580928 0.00583992 0.00589882\n  0.00588197 0.00594108 0.00602648 0.00613527 0.00592755 0.00601652\n  0.00593425 0.00583478 0.00589029 0.00601419 0.00593525 0.00587442\n  0.00590293 0.00581247 0.0057646  0.0057453  0.00597295 0.0058037\n  0.00582562 0.00585909 0.00577982 0.00585903 0.00584188 0.0059149\n  0.00580365 0.00572988 0.00574242 0.00574721 0.00580024 0.00592434\n  0.00589798 0.00584053 0.00573793 0.00586948 0.00594328 0.00576933\n  0.00594867 0.00607409 0.00590215 0.00567281 0.005966   0.0057952\n  0.00584936 0.00575034 0.0060256  0.00604639 0.00579489 0.00597424\n  0.00589158 0.00581212 0.0059935  0.00596958 0.0058435  0.00591583\n  0.0057931  0.00603387 0.00569492 0.00580409 0.00585386 0.00583532\n  0.00575103 0.005912   0.00583897 0.00591926 0.00587665 0.00593539\n  0.00587044 0.00595201 0.00582084 0.00587896 0.00588644 0.00603404\n  0.00588461 0.0058275 ]]\n[[0.00580045 0.00573881 0.00579584 0.00587651 0.00593136 0.00599062\n  0.00563933 0.00609082 0.00578086 0.00570421 0.00605334 0.00585143\n  0.00597741 0.00583663 0.00592357 0.00581788 0.00573053 0.00602573\n  0.00594358 0.00584682 0.00597131 0.00592946 0.00579741 0.00598099\n  0.00581896 0.00592952 0.00604906 0.00576038 0.00567582 0.00581264\n  0.00578008 0.00592932 0.0057108  0.00584931 0.00592555 0.00621425\n  0.00610288 0.00576105 0.00591691 0.00602403 0.00577333 0.0058019\n  0.00591747 0.00597496 0.00597397 0.00591244 0.00590176 0.00581941\n  0.00603254 0.0059263  0.00591673 0.00610798 0.00599369 0.00601243\n  0.00589116 0.00583414 0.00569534 0.00590642 0.00593869 0.005988\n  0.00583198 0.00568979 0.00572786 0.0059444  0.00595811 0.00586621\n  0.00585535 0.00586967 0.00588288 0.00577336 0.00572163 0.00581039\n  0.00585524 0.00594591 0.00587237 0.00599736 0.0059134  0.00583279\n  0.00589262 0.00613675 0.00590491 0.00588968 0.00596451 0.00585719\n  0.00592466 0.0059196  0.00598004 0.00596717 0.00579447 0.00569206\n  0.00590151 0.00598322 0.00579914 0.00582043 0.00582624 0.00589592\n  0.00587263 0.00590807 0.00603108 0.00611469 0.00593932 0.00602268\n  0.00594712 0.00585222 0.00586931 0.00601273 0.00592993 0.00587299\n  0.00592893 0.00578126 0.00575393 0.00574836 0.00596319 0.00581416\n  0.00583824 0.00585231 0.00576701 0.00583913 0.0058615  0.00591285\n  0.00581056 0.00574549 0.00574069 0.00574603 0.00581401 0.00591432\n  0.00588946 0.00582532 0.00573464 0.00588166 0.00596058 0.00579059\n  0.00593444 0.00607269 0.00591761 0.00567294 0.00596428 0.00580027\n  0.00583589 0.00574557 0.00601664 0.00606708 0.00580129 0.0059746\n  0.00591731 0.00578737 0.00595342 0.00597679 0.00580407 0.00593595\n  0.00580931 0.00602927 0.00569555 0.00580872 0.00586666 0.00583445\n  0.00575957 0.00592877 0.00583758 0.00589197 0.00588492 0.00595455\n  0.00585446 0.00597168 0.00581507 0.00586736 0.00587469 0.00601199\n  0.00589526 0.00583004]]\n[[0.00579633 0.00575268 0.00578123 0.00586916 0.00592641 0.00596908\n  0.00565745 0.00609707 0.00579213 0.00573665 0.00605572 0.00583337\n  0.00598808 0.00583862 0.00592691 0.00580488 0.00573517 0.00601066\n  0.00593838 0.00585099 0.00597529 0.00590545 0.00581878 0.00598873\n  0.00582912 0.0059015  0.00605026 0.00575866 0.00567485 0.00580575\n  0.00577338 0.00593903 0.00569916 0.00585764 0.00592608 0.00621732\n  0.00607915 0.00578057 0.00590166 0.00600581 0.00579068 0.00581512\n  0.0059082  0.00596216 0.00596228 0.00590541 0.00591165 0.00583845\n  0.00603874 0.00594015 0.00591949 0.00611244 0.00598452 0.00602335\n  0.00589842 0.00585251 0.00569252 0.00589493 0.00594373 0.00598522\n  0.00583861 0.00568475 0.00573755 0.00595227 0.00597195 0.00587866\n  0.00587428 0.0059106  0.00586474 0.00579298 0.00574753 0.00582678\n  0.00584127 0.00592687 0.00585391 0.00599913 0.00589777 0.00583809\n  0.00588808 0.00614674 0.00589789 0.00586814 0.00593477 0.00585828\n  0.00592065 0.00592581 0.00597774 0.0059489  0.00578366 0.00569855\n  0.00589589 0.00596659 0.00581576 0.00582822 0.00584065 0.00589651\n  0.00588165 0.00589131 0.00601234 0.00610821 0.00593162 0.00601425\n  0.00595139 0.00586475 0.00587726 0.00600738 0.00594782 0.00585818\n  0.0059138  0.0057983  0.00574641 0.00571037 0.00598121 0.00579905\n  0.00581877 0.00586919 0.00577125 0.00583    0.0058438  0.00590807\n  0.00581203 0.00575554 0.00575743 0.00572853 0.00580476 0.00590521\n  0.00589612 0.00581376 0.00574511 0.00586852 0.0059821  0.00579547\n  0.00595215 0.00606193 0.00591628 0.00569182 0.00596236 0.00580676\n  0.0058429  0.00574318 0.00602078 0.00609119 0.00578531 0.00597303\n  0.00592056 0.00579416 0.00595178 0.00595512 0.00579601 0.00592047\n  0.00582513 0.00603133 0.0056878  0.00581443 0.00587403 0.00583524\n  0.00576674 0.00591469 0.00583321 0.00589805 0.00588979 0.00593447\n  0.0058481  0.00598155 0.00581501 0.00590346 0.00587506 0.00600489\n  0.00589519 0.00582538]]\n[[0.00579552 0.00574381 0.00576787 0.00587248 0.00593149 0.00595755\n  0.00568951 0.00607977 0.00581453 0.00574439 0.00605483 0.00583291\n  0.00597625 0.00585985 0.00592673 0.00579874 0.00574005 0.00601338\n  0.00592956 0.00586137 0.00597232 0.00590256 0.00583601 0.00597385\n  0.00583482 0.00589875 0.00606244 0.00577478 0.00568319 0.00579867\n  0.0057823  0.00593192 0.00570926 0.00587088 0.00592833 0.00619531\n  0.00607526 0.00578437 0.00591069 0.00600983 0.00581612 0.00582214\n  0.00590327 0.00595068 0.00595448 0.00588897 0.00592285 0.00582737\n  0.00600865 0.00593768 0.00590227 0.00611062 0.00596035 0.0060325\n  0.00590652 0.00586067 0.00570035 0.00590595 0.00592238 0.00597562\n  0.00584144 0.00568094 0.00573191 0.00595991 0.00594754 0.00587126\n  0.0058657  0.00592403 0.00586973 0.00579584 0.00575741 0.00581569\n  0.00584185 0.00593281 0.0058555  0.00598315 0.00587843 0.00586252\n  0.0058952  0.0061556  0.00589694 0.0058561  0.00594371 0.00586243\n  0.00591455 0.00591874 0.00595736 0.00594397 0.00579429 0.00568744\n  0.0058699  0.00594688 0.00582188 0.00583354 0.00585088 0.00589083\n  0.00587319 0.00585496 0.00601324 0.00608542 0.00595866 0.00601056\n  0.00595157 0.00584313 0.0058671  0.00600092 0.00595449 0.00587381\n  0.00592582 0.00579684 0.00574642 0.0056951  0.00596871 0.00580411\n  0.0058007  0.00586004 0.00579285 0.00579723 0.0058695  0.00591286\n  0.00582112 0.00577117 0.00577763 0.00573566 0.00580323 0.00590644\n  0.0059007  0.00580414 0.00575263 0.00589945 0.00596992 0.00580173\n  0.00594459 0.00605874 0.00589874 0.00569876 0.00597387 0.0058155\n  0.00585481 0.00573625 0.00602447 0.00609586 0.00580948 0.00598384\n  0.00589777 0.0058075  0.0059546  0.00595052 0.00578099 0.00591154\n  0.00585537 0.00602039 0.00570393 0.00582434 0.00587106 0.00583124\n  0.00578026 0.00592738 0.00581596 0.00589123 0.00587236 0.00593246\n  0.00586874 0.00596271 0.00582576 0.00590673 0.00588101 0.00600249\n  0.00592268 0.0058136 ]]\n[[0.00578202 0.00574976 0.00577385 0.00588471 0.00594115 0.00594653\n  0.00571354 0.00608908 0.0057939  0.00573656 0.00605264 0.00583468\n  0.00594523 0.00586358 0.00592094 0.00579714 0.00574984 0.00599889\n  0.00592621 0.00585556 0.00595798 0.00591288 0.0058282  0.00596143\n  0.00584213 0.00592843 0.00606607 0.00577267 0.00568547 0.00577386\n  0.00577757 0.00594174 0.00572922 0.00584734 0.00592563 0.00619959\n  0.00607166 0.00578825 0.00593292 0.00602143 0.00580134 0.00583848\n  0.0059017  0.00595102 0.00595421 0.00588743 0.00592319 0.00584552\n  0.00601139 0.00591307 0.00588911 0.00610446 0.0059623  0.00604123\n  0.0058952  0.00583925 0.00570944 0.00591    0.00591455 0.00595679\n  0.00584989 0.00567202 0.00573135 0.00596904 0.00595233 0.00589208\n  0.0058592  0.00589635 0.00585537 0.00577063 0.00573536 0.00583356\n  0.00584028 0.00593024 0.00586273 0.00596184 0.00589397 0.00585945\n  0.00590999 0.0061692  0.00593742 0.00588364 0.00595356 0.0058707\n  0.00589713 0.00590278 0.00596887 0.00596748 0.00581109 0.00568696\n  0.00587765 0.00594525 0.00581788 0.00583103 0.00587084 0.00590847\n  0.005864   0.00585363 0.00602104 0.00610174 0.00595749 0.0060138\n  0.00592808 0.00584613 0.00588798 0.00601023 0.00596405 0.0058911\n  0.00593721 0.00579703 0.00576201 0.00571425 0.00596176 0.0057979\n  0.00580136 0.0058511  0.0057744  0.00581395 0.00588621 0.00590496\n  0.00580162 0.00574979 0.0057816  0.00576382 0.00581013 0.00590581\n  0.00590651 0.0057908  0.0057398  0.00589028 0.00597878 0.00579224\n  0.00592725 0.00607901 0.00590747 0.00568894 0.00596585 0.0057988\n  0.00588014 0.00571048 0.00601683 0.00608016 0.00581057 0.00596346\n  0.00587773 0.00581083 0.00595427 0.00595379 0.00578097 0.00591963\n  0.00583452 0.00601402 0.00571531 0.00583295 0.00589379 0.0057993\n  0.00578192 0.00593524 0.00583931 0.00589614 0.00588382 0.00592543\n  0.0058902  0.00593574 0.00581238 0.00589304 0.00589046 0.00598841\n  0.00591336 0.00580037]]\n[[0.00578619 0.00574651 0.00578501 0.00588154 0.00593959 0.00592595\n  0.00572043 0.00609305 0.00579467 0.00573995 0.00603667 0.00584664\n  0.00593955 0.00587131 0.00592718 0.00579302 0.0057552  0.00598875\n  0.00591498 0.00585958 0.00595459 0.00590015 0.00584111 0.00596656\n  0.00584131 0.00591687 0.00606313 0.00577739 0.00570308 0.00578291\n  0.00578538 0.00594324 0.0057432  0.0058514  0.00595297 0.00618586\n  0.00606758 0.00580433 0.00592804 0.00600341 0.00579926 0.00584658\n  0.00589165 0.00594873 0.00594884 0.0058966  0.00592198 0.00585468\n  0.00602621 0.00591719 0.00588414 0.0060956  0.00596478 0.00603798\n  0.00589906 0.00584865 0.00571105 0.00590935 0.00591254 0.00595469\n  0.00584056 0.00567577 0.00572772 0.00596825 0.00594919 0.00587607\n  0.00585555 0.00591536 0.00585401 0.00578359 0.00572613 0.00581593\n  0.00582734 0.00590997 0.00586748 0.00595341 0.00589548 0.00586396\n  0.00590789 0.00615929 0.00595867 0.00589044 0.00594399 0.00588877\n  0.00590086 0.00591046 0.00595353 0.00595346 0.00582019 0.00570275\n  0.00588698 0.00592017 0.00582258 0.00582252 0.00586212 0.00590747\n  0.00586668 0.00585664 0.00602161 0.00610089 0.00595519 0.00600584\n  0.00592655 0.0058457  0.00589458 0.00599947 0.00597441 0.00588324\n  0.00592295 0.00579801 0.00576514 0.0057093  0.00596892 0.00579374\n  0.00579017 0.00583788 0.00578452 0.00581066 0.00588223 0.00591316\n  0.00580834 0.00574752 0.00579106 0.00577332 0.00579396 0.00591767\n  0.00590961 0.00579638 0.00575183 0.00587624 0.00595767 0.00579494\n  0.0059311  0.00608526 0.00590234 0.00569191 0.00596735 0.00580632\n  0.00588374 0.00570915 0.00601249 0.0060715  0.00579876 0.00595756\n  0.00586505 0.00583835 0.00596344 0.00594313 0.00577756 0.00591359\n  0.00581904 0.00601779 0.0057149  0.00582809 0.00589812 0.00581726\n  0.00578589 0.00595361 0.00583919 0.00591023 0.00587601 0.00591641\n  0.00590354 0.00592036 0.00581383 0.00590039 0.00590202 0.005992\n  0.00591004 0.00579089]]\n[[0.0057799  0.00574675 0.00578499 0.0058809  0.00593262 0.00591954\n  0.00572758 0.00608495 0.00578285 0.005751   0.00604694 0.00584941\n  0.00593875 0.00587654 0.00592416 0.00578663 0.00576838 0.00598053\n  0.00591591 0.00585208 0.00595547 0.00589132 0.0058543  0.00597276\n  0.00584009 0.00591277 0.00605771 0.0057861  0.00570762 0.00578241\n  0.00577915 0.00594587 0.00574512 0.0058529  0.00596265 0.00617302\n  0.00604895 0.00581547 0.0059205  0.0059915  0.00579671 0.0058614\n  0.00587971 0.00594094 0.00594254 0.00589621 0.00591259 0.00587245\n  0.00602916 0.00592104 0.00589204 0.00608488 0.00596066 0.00602705\n  0.00588578 0.00585676 0.00571864 0.00591369 0.0059098  0.00595698\n  0.00584187 0.00568309 0.00573486 0.00595854 0.00595111 0.00587424\n  0.00587196 0.00591726 0.00584101 0.00578927 0.00571741 0.00581523\n  0.00582504 0.00589322 0.00586785 0.00596104 0.00590016 0.00585442\n  0.00591596 0.0061568  0.00597369 0.00589555 0.00592694 0.00589345\n  0.00589807 0.00592099 0.00594817 0.00595821 0.00581783 0.00571236\n  0.00589144 0.00590885 0.00582526 0.00580725 0.00585797 0.00590457\n  0.0058619  0.00585929 0.00602271 0.00610069 0.0059428  0.00599869\n  0.00591661 0.00584825 0.00591272 0.00599289 0.00598465 0.00588281\n  0.00592085 0.00581395 0.00577627 0.0057163  0.00596749 0.00579664\n  0.00579581 0.00583579 0.00577869 0.00580943 0.00589174 0.00591769\n  0.00580308 0.00574787 0.00580873 0.00577391 0.00579367 0.00592369\n  0.0059187  0.00578771 0.00575679 0.00586943 0.00596425 0.00579657\n  0.00592772 0.00609042 0.00589918 0.00570183 0.00595873 0.00580408\n  0.00590128 0.00571188 0.00600072 0.00606569 0.00578769 0.00594869\n  0.00586192 0.0058422  0.00596834 0.00592887 0.00577764 0.0059161\n  0.00581465 0.00602497 0.00571588 0.00582527 0.00590502 0.00581905\n  0.00578607 0.00595736 0.00584634 0.00591927 0.00588305 0.00590863\n  0.00591095 0.00590733 0.00580248 0.00590265 0.00590845 0.00599637\n  0.00590044 0.00578777]]\n[[0.00579072 0.0057348  0.0057776  0.0058639  0.00595956 0.00592463\n  0.00572633 0.00607348 0.00579323 0.00575819 0.00603988 0.0058625\n  0.00590996 0.00589972 0.005931   0.00578023 0.00576799 0.00600496\n  0.0059065  0.00586598 0.00596269 0.00588685 0.00584806 0.00596324\n  0.00582667 0.00591518 0.00607342 0.00578133 0.00570444 0.00577565\n  0.00579903 0.00595081 0.00575923 0.00585413 0.00597558 0.00615989\n  0.00603826 0.00582386 0.00592311 0.00599379 0.00581212 0.00584703\n  0.00587186 0.00593577 0.00594404 0.00590706 0.00591062 0.00585988\n  0.00601207 0.00590577 0.00587133 0.00607284 0.00595128 0.00602869\n  0.00589138 0.00583718 0.00573803 0.0059292  0.00590287 0.00597348\n  0.00583122 0.0056991  0.00572496 0.00596094 0.00595034 0.00588518\n  0.00584927 0.0059234  0.00584168 0.00576952 0.00570525 0.00579772\n  0.00581891 0.00591195 0.00587754 0.00594463 0.00590046 0.0058643\n  0.00592688 0.00613495 0.00598767 0.00588844 0.00594166 0.00590711\n  0.00589964 0.00592189 0.00594491 0.00593711 0.00581327 0.00572475\n  0.00586853 0.00589642 0.00581791 0.00580478 0.0058731  0.00588032\n  0.00588132 0.00585051 0.00602631 0.00611745 0.00595982 0.00598833\n  0.00591577 0.00583723 0.0059156  0.00597935 0.00597326 0.00589206\n  0.00593647 0.00581418 0.00578585 0.00571246 0.00594385 0.00579317\n  0.00578416 0.00583845 0.00580328 0.00581449 0.00590859 0.00591841\n  0.00580894 0.00577024 0.00579432 0.00577216 0.00578718 0.00593761\n  0.00591383 0.00578489 0.00575576 0.00588244 0.0059387  0.00579783\n  0.00590072 0.00610583 0.00590099 0.00570136 0.00596016 0.00579726\n  0.00591532 0.00571999 0.00599147 0.00606158 0.00581661 0.0059373\n  0.0058508  0.00584469 0.0059671  0.0059367  0.00577547 0.00591282\n  0.00581913 0.00601683 0.00572277 0.00583836 0.00588685 0.00582172\n  0.00578238 0.00596685 0.00584624 0.00594181 0.00587009 0.00589851\n  0.00591303 0.00590494 0.00582503 0.00589695 0.00590793 0.00600306\n  0.00591055 0.00578204]]\n[[0.00577428 0.00573346 0.00577131 0.00585992 0.00596285 0.00591476\n  0.00572666 0.00607983 0.00579055 0.00576141 0.00604237 0.00586926\n  0.00592966 0.00587386 0.00592369 0.00578118 0.00575785 0.00601779\n  0.00589575 0.00585259 0.00594818 0.0058853  0.00585559 0.00597475\n  0.00580528 0.00591452 0.00606922 0.00578277 0.00570907 0.00577944\n  0.00579814 0.0059468  0.00573106 0.00586093 0.00598152 0.00618989\n  0.00603859 0.00581432 0.00590014 0.0059902  0.00578791 0.00585563\n  0.005857   0.00594983 0.00593427 0.0059138  0.00590249 0.00586057\n  0.00603675 0.00591708 0.00587502 0.0060857  0.00596256 0.00601949\n  0.00587156 0.00585743 0.00572    0.00593753 0.00591109 0.0059904\n  0.0058303  0.00567366 0.00572259 0.00596181 0.00595964 0.00587798\n  0.00586132 0.00591342 0.00582787 0.00576724 0.00571662 0.00582245\n  0.00580977 0.0059033  0.00587644 0.00595256 0.0058994  0.00585598\n  0.00592516 0.00612654 0.00596993 0.00591047 0.00591909 0.00588754\n  0.00588554 0.00593779 0.00595586 0.00595724 0.00581268 0.00571277\n  0.00588098 0.0059016  0.00580481 0.00578541 0.00584057 0.00588633\n  0.00588399 0.00586871 0.00602226 0.00613057 0.00596432 0.00599996\n  0.00592757 0.00585864 0.00593544 0.00598764 0.00598864 0.00588311\n  0.00592695 0.00581306 0.00577891 0.00571822 0.00594676 0.00579026\n  0.00579245 0.00584096 0.00577346 0.00582449 0.00590215 0.00592653\n  0.00580071 0.00577952 0.00579226 0.00577211 0.00578102 0.00593556\n  0.00591884 0.00576727 0.00576071 0.00586917 0.00596211 0.00582126\n  0.00591199 0.00612007 0.00591724 0.0057062  0.00595502 0.00579596\n  0.00591732 0.00570128 0.00599933 0.00607826 0.00579373 0.00596307\n  0.00586615 0.00585973 0.00596511 0.00591687 0.00575768 0.0059284\n  0.00580229 0.00604146 0.00570801 0.00582612 0.00590406 0.00580236\n  0.00577242 0.00596535 0.00584549 0.00594885 0.00589079 0.0059067\n  0.00589262 0.00592597 0.00580877 0.00589718 0.00590585 0.00600042\n  0.00590342 0.00577928]]\n"
    }
   ],
   "source": [
    "first_half = generate_class(lstm_model, tokenizer, embeddings, peak_pose, \"easy pose\", 40)[::-1]\n",
    "second_half = generate_class(lstm_model, tokenizer, embeddings, peak_pose, \"corpse pose\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5359453e-03\n  5.46163472e-04 2.93880911e-03 1.10193517e-03 7.33044581e-04\n  1.80510653e-03 5.43511764e-04 4.68226412e-04 2.97679740e-04\n  1.12930298e-04 5.12780134e-05 9.81213525e-05 8.08024488e-05\n  5.94915077e-03 3.87635591e-05 2.51954643e-05 5.58512926e-04\n  4.05777931e-01 2.63976370e-04 7.15289358e-03 1.47281011e-04\n  3.54484329e-03 1.80578045e-05 3.92725597e-05 2.62501859e-03\n  1.30023880e-04 1.30949236e-04 1.57890157e-04 4.64949670e-04\n  8.06473661e-04 4.17996012e-03 1.09793497e-02 5.83300460e-03\n  7.61924917e-03 2.21049049e-04 3.44949990e-06 3.01657801e-05\n  2.20681377e-05 4.69884631e-04 2.58192940e-05 8.93915188e-04\n  9.84599814e-04 3.42842977e-05 5.18304569e-06 1.93204207e-04\n  1.09261819e-04 1.82086369e-03 7.58035821e-05 3.49120208e-04\n  2.13424186e-03 3.43178457e-04 1.90332241e-04 8.50396918e-06\n  4.43343772e-04 1.48926559e-03 1.10252637e-04 1.02564136e-05\n  3.94082599e-05 2.80361201e-05 2.46423224e-05 6.42651912e-06\n  1.69019386e-05 3.71971055e-06 2.38505047e-04 1.38644264e-05\n  2.13056407e-03 1.13800459e-03 7.95136672e-03 9.63256578e-04\n  4.65556841e-05 9.15952842e-04 2.66727202e-05 6.80607263e-05\n  7.18293950e-06 1.83758006e-04 3.27869726e-04 1.24373346e-05\n  4.46353115e-05 4.59215633e-04 1.84214459e-05 7.72898609e-04\n  7.35642107e-06 3.78801808e-04 6.85785722e-04 3.86228814e-04\n  8.71465163e-05 1.44588630e-04 3.62604733e-05 1.15684188e-05\n  3.58022917e-05 3.79455081e-08 7.71759824e-06 3.25327483e-06\n  2.87301173e-05 9.06555033e-06 1.66713326e-05 1.15160783e-05\n  1.01111627e-05 1.30002372e-05]]\n[[1.25678301e-10 8.40641651e-03 1.11722248e-02 1.56357384e-03\n  5.22063440e-03 4.10159566e-02 5.09441365e-03 1.66377216e-03\n  3.37801035e-03 1.02638581e-03 2.94442335e-03 1.79720548e-04\n  1.41341949e-03 9.83091146e-02 6.63108425e-04 5.93723264e-04\n  4.69095595e-02 1.07138984e-01 4.45835292e-04 3.73103819e-03\n  3.73139192e-04 2.52479687e-04 8.93523276e-04 1.59572531e-02\n  1.84212681e-02 3.76289070e-04 2.00410268e-05 3.07833124e-02\n  1.22104716e-02 2.09794450e-03 7.21908547e-03 6.02559339e-05\n  2.42220238e-02 9.08637233e-03 2.57383945e-04 7.67731399e-04\n  6.31480943e-05 1.85066613e-03 1.01222377e-03 2.89712392e-04\n  2.60965116e-02 4.98023815e-03 6.92388136e-03 1.67697184e-02\n  6.22716791e-04 6.41052029e-04 4.47452214e-04 1.22310870e-04\n  2.47865450e-03 5.55540377e-04 3.18853520e-02 5.89602403e-02\n  1.09641649e-01 7.10490684e-04 2.06917152e-02 3.79432822e-05\n  1.45322131e-02 1.33844826e-03 3.25508020e-03 1.22350198e-03\n  1.06733607e-03 4.59332914e-05 3.84106138e-03 2.52253230e-04\n  8.19707848e-03 3.93766561e-04 9.27561719e-04 1.57902029e-03\n  4.40295041e-03 7.86694698e-03 3.62916733e-04 4.28651907e-02\n  3.52187635e-04 8.51629209e-03 5.46700379e-04 7.60160328e-04\n  8.95582512e-03 5.84948633e-04 4.69055609e-04 1.72008615e-04\n  1.32758840e-04 3.21083906e-04 1.35750321e-04 4.07303814e-05\n  5.39363921e-03 6.93147449e-05 1.32222413e-05 4.01230529e-04\n  4.66890372e-02 1.06203886e-04 3.06870975e-03 3.15386744e-04\n  1.96591802e-02 5.28263809e-05 8.55099206e-05 6.42068963e-03\n  1.17315518e-04 1.24876125e-04 8.37360203e-05 2.16160915e-04\n  1.60152710e-03 3.30086215e-03 9.05963313e-03 8.71541817e-03\n  1.82667677e-03 4.55968548e-04 6.14453456e-05 9.51266265e-05\n  3.20802501e-05 6.34649163e-03 3.10725518e-05 9.99649405e-04\n  1.20618695e-03 9.35098797e-05 1.42961071e-05 6.75293195e-05\n  3.38575745e-04 1.56064052e-03 1.47780593e-04 2.33748625e-03\n  6.01983571e-04 3.20889987e-04 1.61263320e-04 1.67665567e-05\n  5.47173200e-04 9.12660151e-04 1.13356175e-04 4.22041921e-05\n  8.23862574e-05 1.23431280e-04 1.39490849e-05 1.66217269e-05\n  2.17414909e-05 1.83284355e-05 3.97355216e-05 5.17650687e-06\n  3.92677757e-04 4.09105269e-04 8.94927885e-03 3.34920129e-04\n  2.89088115e-04 4.24518046e-04 2.59822009e-05 5.02527546e-05\n  1.77108213e-05 2.58406188e-04 1.26593036e-03 8.88601426e-05\n  2.08824604e-05 2.88621202e-04 2.21990013e-06 1.89028768e-04\n  7.13869958e-05 8.80535008e-05 3.52154049e-04 1.29580192e-04\n  5.20657959e-05 7.98225301e-05 1.78878327e-04 7.32035960e-06\n  1.38502901e-05 1.01856449e-06 4.59610055e-06 2.21972132e-05\n  1.34575521e-05 1.15557959e-05 4.80634299e-06 6.80660924e-06\n  5.63401591e-06 1.38171627e-05]]\n[[1.46535895e-15 5.49922406e-04 4.24905587e-03 5.79947664e-04\n  1.59142620e-03 1.85920112e-02 2.67416937e-03 7.42521661e-04\n  1.99981587e-04 3.02949396e-04 2.91061704e-04 1.31857387e-05\n  2.27751152e-04 6.78381696e-03 6.46192493e-05 5.40763176e-05\n  1.38372127e-02 1.19201047e-02 3.65844433e-04 5.31778205e-04\n  1.78156930e-04 7.15719807e-06 5.46489864e-05 1.92975672e-03\n  2.58344901e-03 3.14787903e-05 5.94197445e-06 9.51965991e-03\n  1.19231315e-03 2.09759470e-04 5.91170392e-04 6.25707889e-06\n  3.71545437e-03 2.35978630e-03 3.68057335e-05 4.03038430e-04\n  1.24096159e-05 7.00625649e-04 8.52714176e-04 2.15376203e-05\n  6.38566958e-03 4.77525551e-04 1.25772713e-04 4.90487926e-03\n  2.04642693e-05 1.43535004e-03 8.59090287e-05 5.03553065e-06\n  1.60655635e-03 2.68873646e-05 4.32231635e-01 7.78173096e-03\n  3.68020892e-01 3.22077758e-05 1.13338674e-03 2.87929265e-06\n  2.91633955e-03 2.53375456e-05 1.69545601e-04 3.05535737e-04\n  1.29188760e-04 7.07958804e-07 5.14518877e-04 1.91000741e-04\n  1.98642301e-04 2.91226053e-04 1.55196336e-04 3.34861106e-04\n  7.12550609e-05 3.74939963e-02 1.41644370e-04 1.45573961e-03\n  2.22105271e-04 1.42654125e-03 5.01893483e-05 4.24627680e-04\n  1.70093394e-04 4.61892632e-04 4.84603916e-05 3.72634804e-06\n  8.00364342e-06 9.40643877e-05 8.65181903e-07 1.37557772e-05\n  1.60375505e-03 1.04421733e-05 3.16417726e-07 5.59156142e-05\n  8.46281741e-03 1.74799970e-05 1.08249132e-02 4.68391818e-06\n  1.00831327e-03 7.68000973e-06 5.28351848e-05 4.31470282e-04\n  3.47564765e-06 1.18342259e-05 9.21450101e-06 1.09946413e-04\n  2.36103675e-04 8.79346859e-04 2.40367535e-03 6.25172025e-03\n  1.06415036e-03 3.65369192e-06 2.11872361e-07 4.53554057e-05\n  9.01654914e-07 8.15255917e-05 1.67651669e-05 2.99411786e-05\n  8.95213525e-05 2.34435583e-06 2.96489873e-07 1.76165804e-06\n  1.91242952e-06 1.01816608e-03 2.39232031e-06 4.13619127e-05\n  7.37858784e-07 4.42718047e-05 8.19719935e-05 9.67859160e-06\n  1.16402798e-05 7.45289552e-04 1.46446735e-04 5.78714719e-07\n  2.16207627e-05 2.66259531e-06 7.16287559e-07 1.74148372e-05\n  1.20952635e-07 1.15567173e-06 8.92989647e-07 5.66635549e-07\n  2.25535241e-05 2.78295629e-04 4.12714493e-04 2.05270135e-05\n  8.93938923e-05 2.75096926e-03 3.60840374e-07 6.02159935e-06\n  7.44339786e-07 1.09684894e-04 4.17589181e-04 1.37648908e-06\n  3.64210573e-05 2.34778909e-05 7.99603654e-08 1.63926488e-05\n  5.37206251e-05 8.62307047e-07 1.74497836e-05 1.70413659e-05\n  4.60323172e-05 3.63777167e-06 8.39092991e-06 2.83932707e-07\n  8.50870219e-06 1.90040268e-08 5.28447090e-05 4.56800564e-07\n  1.02761420e-04 6.27408463e-06 1.01786641e-08 4.36220253e-05\n  9.81825021e-09 2.56248605e-08]]\n[[8.36246406e-15 6.29725342e-04 7.18736276e-03 1.12531008e-03\n  3.25242826e-03 2.22782847e-02 3.83737870e-03 1.06921024e-03\n  4.57543531e-04 1.02953333e-03 2.71749741e-04 2.50987923e-05\n  3.93797236e-04 1.06833596e-02 8.33645317e-05 9.83716018e-05\n  2.16872338e-02 1.45056732e-02 7.14919297e-04 1.06167898e-03\n  3.64084146e-04 1.55093639e-05 1.00302619e-04 2.33545667e-03\n  3.38833849e-03 4.74695153e-05 1.49336838e-05 9.71266441e-03\n  1.07603054e-03 4.12088906e-04 6.09005569e-04 1.34612956e-05\n  3.46214697e-03 3.11299041e-03 5.49396063e-05 6.98747928e-04\n  2.95605205e-05 9.20605497e-04 8.01431772e-04 2.77529653e-05\n  4.39959299e-03 7.75397639e-04 2.02142852e-04 4.42382740e-03\n  5.22364935e-05 2.18161731e-03 2.12057523e-04 8.71568500e-06\n  3.21425288e-03 4.58792529e-05 3.11270714e-01 6.32198201e-03\n  4.05819297e-01 4.65274061e-05 1.25629990e-03 8.04741831e-06\n  4.14820435e-03 3.06182992e-05 2.70893826e-04 4.47330211e-04\n  2.11004095e-04 1.48785637e-06 7.37810915e-04 4.37968905e-04\n  2.27496945e-04 7.10848311e-04 3.65914922e-04 5.72630961e-04\n  1.01658341e-04 7.67248496e-02 2.80735374e-04 1.30995130e-03\n  4.11621731e-04 2.59961211e-03 6.92991889e-05 6.32853247e-04\n  2.46750191e-04 1.06509868e-03 1.26825471e-04 6.77602384e-06\n  1.30236149e-05 1.96399385e-04 1.48070228e-06 3.44537839e-05\n  2.92420690e-03 2.11802962e-05 5.96294967e-07 8.79788058e-05\n  1.00431805e-02 3.32628333e-05 1.35533139e-02 7.36567927e-06\n  1.12402090e-03 1.65140100e-05 1.19927507e-04 3.40620492e-04\n  1.07493461e-05 2.37614713e-05 2.80887853e-05 2.70196935e-04\n  4.84008546e-04 8.76999868e-04 2.03309441e-03 6.15907973e-03\n  1.22846768e-03 6.40086546e-06 6.90596323e-07 1.01496677e-04\n  1.35944026e-06 1.42693025e-04 3.75294949e-05 5.08978337e-05\n  1.59482035e-04 6.16113221e-06 7.04202364e-07 4.05547053e-06\n  4.05411311e-06 2.76919478e-03 5.58015563e-06 5.37499109e-05\n  1.05532183e-06 6.54775140e-05 1.45862577e-04 5.35556283e-05\n  2.16716999e-05 7.92493753e-04 3.27154121e-04 1.06277548e-06\n  4.00775862e-05 8.18633998e-06 1.22030383e-06 3.40327424e-05\n  3.06841542e-07 4.10419489e-06 1.58892533e-06 1.26829821e-06\n  3.71773640e-05 3.73186922e-04 3.02447501e-04 2.36981068e-05\n  1.67972641e-04 3.44977784e-03 8.30381680e-07 1.26942905e-05\n  2.23230177e-06 1.77365146e-04 5.87429677e-04 3.61925277e-06\n  9.05826892e-05 3.26757327e-05 1.38495707e-07 2.09887912e-05\n  8.62218658e-05 9.47837407e-07 1.76171179e-05 2.40865447e-05\n  7.88188772e-05 4.43582258e-06 1.29655928e-05 3.02191012e-07\n  1.87256064e-05 5.21569632e-08 1.19295568e-04 1.32925891e-06\n  1.88034071e-04 2.16976114e-05 2.05470663e-08 1.01219346e-04\n  1.68881833e-08 2.62600075e-08]]\n[[1.49098382e-14 7.57936796e-04 8.65472574e-03 2.32421956e-03\n  4.55358997e-03 1.92870181e-02 4.72209230e-03 1.56099815e-03\n  6.30360271e-04 2.00058310e-03 2.91724369e-04 4.47974926e-05\n  5.62732399e-04 1.19646350e-02 1.16629053e-04 1.54631925e-04\n  2.39139851e-02 1.52914710e-02 1.12763129e-03 1.20387797e-03\n  7.67368241e-04 2.14978245e-05 1.52679189e-04 2.71056918e-03\n  4.23145341e-03 5.95874953e-05 3.07734517e-05 1.07694035e-02\n  9.60444217e-04 6.81745703e-04 5.99999214e-04 1.97421723e-05\n  3.48338019e-03 3.99675546e-03 8.69879223e-05 1.00793072e-03\n  4.53778812e-05 1.01367303e-03 8.77799001e-04 4.35024449e-05\n  3.64250387e-03 9.77772288e-04 2.11844526e-04 3.97218717e-03\n  8.13992374e-05 3.89328389e-03 3.88067856e-04 1.24860971e-05\n  3.87997739e-03 5.50706027e-05 2.10324943e-01 3.86479218e-03\n  4.81571406e-01 7.02161851e-05 9.20885766e-04 1.52608209e-05\n  3.73740657e-03 6.63172395e-05 2.75082130e-04 5.69247350e-04\n  2.36497173e-04 2.83411168e-06 5.95441263e-04 6.16982987e-04\n  3.00400832e-04 1.43529742e-03 7.24349695e-04 6.69139437e-04\n  1.52505134e-04 8.04987177e-02 4.62177355e-04 1.18176884e-03\n  6.03066059e-04 3.53742740e-03 8.34112943e-05 7.06297986e-04\n  3.18982202e-04 1.94305601e-03 2.21981725e-04 1.78467290e-05\n  1.68173665e-05 2.22009359e-04 1.97151826e-06 6.69059955e-05\n  3.47240665e-03 4.13254711e-05 1.42415422e-06 1.55653994e-04\n  1.33178188e-02 5.67048337e-05 1.42340520e-02 9.16271711e-06\n  9.98912496e-04 2.28702411e-05 2.71001598e-04 3.13183118e-04\n  2.31478189e-05 4.06542931e-05 5.63130998e-05 6.12734351e-04\n  8.04495299e-04 9.86241619e-04 2.84589082e-03 4.68832115e-03\n  1.49868755e-03 8.56019415e-06 1.39923497e-06 1.23029502e-04\n  2.05719130e-06 1.36907722e-04 5.52918864e-05 5.59700638e-05\n  3.05219204e-04 1.09757630e-05 1.02270292e-06 5.79936341e-06\n  6.99306656e-06 3.03949276e-03 9.72154885e-06 5.40557776e-05\n  1.09275572e-06 8.67880517e-05 2.92512763e-04 1.51611574e-04\n  3.50259070e-05 9.20130813e-04 5.14363579e-04 1.62877905e-06\n  5.82011016e-05 1.22046295e-05 1.76846129e-06 7.39097377e-05\n  6.35326899e-07 5.44381646e-06 2.15383420e-06 1.88247770e-06\n  5.95470519e-05 4.40003612e-04 4.05210798e-04 2.96683538e-05\n  1.55244968e-04 2.75836233e-03 1.35522043e-06 2.06975928e-05\n  4.19404523e-06 1.86038596e-04 6.18769263e-04 5.60130229e-06\n  2.00063441e-04 2.74882423e-05 2.10332573e-07 3.22241685e-05\n  6.81704405e-05 9.87680210e-07 1.81586074e-05 3.07005248e-05\n  9.14397970e-05 4.40248323e-06 1.20215809e-05 3.42119847e-07\n  4.47682360e-05 7.31560448e-08 1.54682362e-04 1.33970502e-06\n  1.86917649e-04 3.05922658e-05 3.32201502e-08 1.13456132e-04\n  2.51163801e-08 2.62605102e-08]]\n[[1.95984705e-14 8.29790777e-04 9.16842930e-03 3.04432400e-03\n  4.80445335e-03 1.64528415e-02 5.23241702e-03 1.73050992e-03\n  7.20988377e-04 2.87323864e-03 3.12323769e-04 6.01119391e-05\n  6.45850203e-04 1.10665578e-02 1.38270698e-04 2.02018637e-04\n  2.20962763e-02 1.34267658e-02 1.45399396e-03 1.16237521e-03\n  1.11483608e-03 2.05989636e-05 1.79330222e-04 2.90144212e-03\n  4.36764350e-03 6.53682364e-05 4.55127738e-05 1.09108360e-02\n  7.37182505e-04 8.81775923e-04 5.46098920e-04 2.63811708e-05\n  3.40879173e-03 4.21521300e-03 1.02572150e-04 1.16821902e-03\n  6.01698084e-05 1.02824788e-03 8.41561297e-04 4.69358783e-05\n  3.43039189e-03 1.03191216e-03 1.96996465e-04 3.57847987e-03\n  9.19753365e-05 5.88155678e-03 5.44801820e-04 1.42825893e-05\n  3.87384114e-03 5.53310456e-05 1.52465716e-01 2.37611774e-03\n  5.54904282e-01 7.67161764e-05 6.74438896e-04 2.25071981e-05\n  3.17668286e-03 9.71760746e-05 2.64108647e-04 6.19656756e-04\n  2.36026332e-04 4.60946285e-06 4.63866716e-04 7.94394873e-04\n  3.34423239e-04 2.23321747e-03 9.68299573e-04 6.72132825e-04\n  1.75316920e-04 6.41440973e-02 6.15227502e-04 9.94561706e-04\n  6.83419872e-04 4.19511739e-03 9.02377578e-05 7.11031666e-04\n  3.48481844e-04 2.52396055e-03 2.82860710e-04 2.77651798e-05\n  1.76795002e-05 2.24251926e-04 2.17824709e-06 1.02804152e-04\n  3.71793355e-03 5.31697988e-05 2.40945405e-06 1.96911380e-04\n  1.38564715e-02 6.66280175e-05 1.45480549e-02 9.40125938e-06\n  9.08078393e-04 2.32549846e-05 4.57461458e-04 2.53195118e-04\n  3.30852381e-05 5.31225123e-05 7.43382770e-05 9.41965787e-04\n  9.38623620e-04 1.02084409e-03 3.28230090e-03 3.67220095e-03\n  1.54757511e-03 8.87212809e-06 1.78416713e-06 1.31004985e-04\n  2.44294347e-06 1.16981544e-04 6.65284606e-05 5.08268931e-05\n  3.66447988e-04 1.43893249e-05 1.19654987e-06 7.88418765e-06\n  9.48304842e-06 2.89746746e-03 1.24706175e-05 4.17470292e-05\n  1.07544213e-06 9.03902037e-05 4.37073264e-04 2.43758608e-04\n  3.89590350e-05 9.17869911e-04 6.50593196e-04 2.11423776e-06\n  6.46003828e-05 1.35461423e-05 1.97393274e-06 1.02802187e-04\n  8.94646746e-07 5.29508179e-06 2.49590357e-06 2.24226255e-06\n  6.85391715e-05 4.80287970e-04 3.97864089e-04 2.92956320e-05\n  1.28329455e-04 2.61630584e-03 1.68612041e-06 2.22746166e-05\n  5.75303238e-06 1.67250604e-04 6.20225503e-04 6.89569697e-06\n  3.16123653e-04 2.23391617e-05 2.65415679e-07 3.67272951e-05\n  5.23436320e-05 9.04962121e-07 1.72270720e-05 3.14100289e-05\n  9.64516730e-05 4.06865911e-06 9.88252032e-06 3.47115389e-07\n  6.36693076e-05 7.70300517e-08 1.81108888e-04 1.06045036e-06\n  1.85245939e-04 3.25132060e-05 3.95863431e-08 1.28041196e-04\n  2.76768670e-08 2.22996839e-08]]\n[[4.98494535e-09 1.53780531e-03 1.48869921e-02 1.21824024e-03\n  3.55071388e-03 7.70216510e-02 5.32515161e-03 1.48676394e-03\n  5.16370684e-03 4.71445220e-03 5.91893506e-04 3.34884651e-04\n  1.78919162e-03 4.48178440e-01 6.73321483e-04 9.92677175e-04\n  1.39544662e-02 7.94157833e-02 2.77084531e-04 4.10808297e-03\n  1.12004054e-03 3.81245802e-04 5.85452828e-04 8.47463682e-03\n  4.48864931e-03 4.08265711e-04 7.47953018e-05 6.09114114e-03\n  1.73634593e-03 1.40472373e-03 1.67837937e-03 6.69139787e-04\n  5.25417505e-03 3.18562100e-03 1.49521074e-04 5.23325230e-04\n  1.27413470e-04 6.26094872e-03 4.58236202e-04 6.35285513e-04\n  8.26300215e-03 2.97970208e-03 3.87613336e-03 5.44960471e-03\n  2.59530032e-03 9.31066752e-04 1.32599031e-03 2.38790046e-04\n  4.65086102e-03 6.55301614e-04 1.27971983e-02 2.94773281e-02\n  1.11575965e-02 9.05155786e-04 9.76260379e-03 8.67848430e-05\n  1.03436094e-02 2.29053013e-03 6.70579378e-04 1.24280585e-03\n  2.62977229e-03 3.24001921e-05 1.85150444e-03 8.91777221e-04\n  1.02707313e-03 3.11391021e-04 2.25364277e-03 9.74830985e-03\n  3.15842591e-03 3.24947909e-02 4.74953646e-04 1.11519229e-02\n  3.39214341e-04 2.59693060e-02 3.52867064e-04 4.82612551e-04\n  3.46704968e-03 5.71372686e-04 5.63603884e-04 2.20959817e-04\n  2.66265066e-04 3.29951290e-04 6.58127174e-05 4.99649250e-05\n  1.52357500e-02 1.73155349e-05 2.30600835e-05 1.55891554e-04\n  8.57621804e-03 1.17433767e-04 7.06040766e-03 1.66832004e-04\n  2.91475374e-03 2.26896154e-04 1.51347078e-04 8.52707482e-04\n  2.09952748e-04 1.29707216e-04 3.16529186e-04 4.24779631e-04\n  1.15224742e-03 2.70964927e-04 2.28620041e-03 6.60423283e-03\n  5.06437966e-04 5.65172581e-04 3.73806630e-04 4.65623052e-05\n  3.23686254e-05 1.31975869e-02 3.09665229e-05 2.72271485e-04\n  3.80926393e-03 4.45375626e-04 1.39137146e-05 2.69539683e-04\n  2.89615156e-04 2.40442320e-03 1.00203026e-04 1.30034940e-04\n  2.23010022e-04 1.00080397e-04 1.90260762e-04 4.11382847e-04\n  3.35753523e-03 5.65206574e-04 9.05382331e-05 2.43457500e-04\n  2.11719416e-05 6.53062540e-04 3.47170644e-05 8.60913678e-06\n  2.86236245e-05 2.85205169e-05 1.50068052e-04 9.66020252e-06\n  5.39782806e-04 1.37103803e-03 1.58394675e-03 5.98740298e-05\n  2.58805958e-04 2.87003408e-04 2.24649702e-05 6.60481237e-05\n  4.50625594e-05 2.34326013e-04 8.21369002e-04 3.63121973e-04\n  3.37899401e-05 2.40528534e-04 2.79022902e-06 9.49833848e-05\n  6.85353662e-05 2.61985697e-05 1.12828697e-04 7.72750936e-05\n  2.16070199e-04 2.54287279e-05 1.11981419e-04 4.77125923e-06\n  6.03395665e-06 1.97711825e-06 2.20959791e-05 9.97583120e-05\n  3.37555757e-05 1.14480084e-04 1.17489893e-04 8.37736370e-05\n  1.42905837e-05 8.19211346e-06]]\n[[2.59761324e-09 3.24294157e-03 2.56864019e-02 1.72322197e-03\n  7.38959527e-03 1.86876118e-01 8.98679253e-03 1.59252854e-03\n  5.93291689e-03 6.84534293e-03 1.02736091e-03 4.74038156e-04\n  3.01851938e-03 2.04783902e-01 5.70690783e-04 1.68503483e-03\n  2.27107760e-02 1.61317855e-01 4.25252802e-04 6.35354780e-03\n  1.52738614e-03 5.98193728e-04 1.50200492e-03 8.03909078e-03\n  7.24923098e-03 8.03323055e-04 4.27076593e-05 7.76189193e-03\n  2.34720064e-03 3.66816949e-03 2.85205687e-03 5.24880073e-04\n  9.43455193e-03 3.46268900e-03 2.29950107e-04 7.31475826e-04\n  1.50439519e-04 6.10803859e-03 2.91795732e-04 5.03518037e-04\n  1.56556349e-02 4.82760929e-03 4.36234474e-03 5.08216582e-03\n  3.18091526e-03 4.81262367e-04 1.11568440e-03 4.66715806e-04\n  7.86930136e-03 8.81474174e-04 1.00825271e-02 3.55541892e-02\n  1.37762725e-02 1.46560348e-03 7.13828998e-03 7.02440448e-05\n  1.84025057e-02 1.64619973e-03 6.43560954e-04 1.71113142e-03\n  2.95557734e-03 4.95903478e-05 1.78882899e-03 1.19039824e-03\n  1.90885761e-03 2.88056995e-04 1.80277810e-03 6.84610847e-03\n  5.26168849e-03 1.83912087e-02 4.34208749e-04 1.01359449e-02\n  5.44476963e-04 1.62187610e-02 2.59108812e-04 1.25880248e-03\n  6.02700328e-03 3.18286475e-04 7.16793002e-04 2.37341388e-04\n  5.43411472e-04 4.98231733e-04 9.12586765e-05 7.68314640e-05\n  8.92956275e-03 1.47147639e-05 2.78362113e-05 1.64076657e-04\n  1.13756927e-02 5.87823743e-05 3.08608450e-03 2.88756710e-04\n  4.03025793e-03 3.56424513e-04 1.09044158e-04 7.70506391e-04\n  1.34566319e-04 1.20236444e-04 2.29013662e-04 5.85850677e-04\n  1.86557369e-03 6.65112166e-04 1.88419409e-03 9.01057571e-03\n  7.62575539e-04 6.59377547e-04 2.83408066e-04 8.59610009e-05\n  7.32925764e-05 1.70926321e-02 5.07744808e-05 2.52423051e-04\n  2.29249336e-03 2.98721134e-04 1.36249573e-05 3.77070479e-04\n  2.09488615e-04 1.77505706e-03 1.17996635e-04 2.46782118e-04\n  4.09329194e-04 9.75596849e-05 1.36068629e-04 2.27230586e-04\n  2.50043417e-03 6.46719302e-04 7.35559661e-05 1.35960829e-04\n  2.47652897e-05 4.68461774e-04 2.37894874e-05 3.02692933e-06\n  3.68054207e-05 4.82377291e-05 1.79868395e-04 6.67951872e-06\n  3.44016997e-04 5.98044018e-04 1.90673431e-03 9.72520647e-05\n  2.58887245e-04 1.72857690e-04 3.46234701e-05 6.60376245e-05\n  3.63438921e-05 2.67465133e-04 8.79219559e-04 2.66187213e-04\n  2.16813041e-05 1.19566481e-04 3.85186877e-06 1.03314014e-04\n  1.05117506e-04 3.55820448e-05 8.38668711e-05 4.41967568e-05\n  9.27877336e-05 3.11199183e-05 9.93762005e-05 1.59818296e-06\n  2.89364289e-06 2.02314595e-06 5.11465305e-06 1.27793377e-04\n  1.35568062e-05 6.27964982e-05 2.63767488e-05 3.10519681e-05\n  5.32239119e-06 7.67962956e-06]]\n"
    }
   ],
   "source": [
    "first_half = generate_class(some_model, tokenizer, embeddings, peak_pose, \"easy pose\", 40)[::-1]\n",
    "second_half = generate_class(some_model, tokenizer, embeddings, peak_pose, \"corpse pose\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "32"
     },
     "metadata": {},
     "execution_count": 371
    }
   ],
   "source": [
    "yoga_class = first_half + second_half\n",
    "class_length = len(yoga_class)\n",
    "class_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Easy Pose',\n 'Corpse Pose',\n 'Plough Pose',\n 'Shoulderstand Pose',\n 'Plough Pose',\n 'Half Lord Of The Fishes Pose',\n 'Thread The Needle Pose',\n 'Thread The Needle Pose, Repeat Other Side',\n 'Low Lunge Pose',\n 'Revolved Extended Side Angle Pose',\n 'Revolved Extended Side Angle Pose, Repeat Other Side',\n 'Figure Four Pose',\n 'Figure Four Pose',\n 'Table Top Pose',\n 'Chaturanga Dandasana',\n 'Upward Facing Dog Pose',\n 'Downward Facing Dog Pose',\n 'Upward Facing Dog Pose',\n 'Locust Pose',\n 'Bow Pose',\n 'Locust Pose',\n 'Upward Facing Dog Pose',\n 'Wheel Pose',\n 'Wheel Pose, Repeat Other Side',\n 'Wind Release Pose',\n 'Shoulderstand Pose',\n 'Shoulderstand Pose, Repeat Other Side',\n 'Shoulderstand Pose',\n 'Shoulderstand Pose, Repeat Other Side',\n 'Supine Spinal Twist Pose',\n 'Supine Spinal Twist Pose, Repeat Other Side',\n 'Corpse Pose']"
     },
     "metadata": {},
     "execution_count": 372
    }
   ],
   "source": [
    "yoga_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling\n",
    "Saving all the models and functions for use in flask app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"word_embeddings.pkl\", \"wb\")\n",
    "pickle.dump(embeddings, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"tokenizer.pkl\", \"wb\")\n",
    "pickle.dump(tokenizer, f)\n",
    "f.close\n",
    "\n",
    "f = open(\"peak_poses.pkl\", \"wb\")\n",
    "pickle.dump(possible_peak_poses, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"peak_pose_dict.pkl\", \"wb\")\n",
    "pickle.dump(peak_pose_dict, f)\n",
    "f.close()"
   ]
  }
 ]
}